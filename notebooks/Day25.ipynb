{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 25: CNNのアーキテクチャ（LeNet, AlexNet）\n",
    "\n",
    "## Learning Objectives\n",
    "- 古典的なCNNアーキテクチャを理解する\n",
    "- 層の深さとパフォーマンスの関係を把握する\n",
    "- パラメータ最適化の技術を学ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Theory (2 hours)\n",
    "\n",
    "## 25.1 CNNの誕生と歴史的背景\n",
    "\n",
    "CNN（畳み込みニューラルネットワーク）は、画像認識の分野で革命的な進化を遂げたアーキテクチャでございます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.1.1 ネオコグニトロンの概念（1980年代）\n",
    "\n",
    "福島邦彦博士による初期の視覚モデルで、以下の概念が生まれました：\n",
    "\n",
    "- **S細胞**: 空間的な結合（畳み込みの原型）\n",
    "- **C細胞**: 不変性の獲得（プーリングの原型）\n",
    "- 層化された構造による特徴抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.1.2 現代CNNの確立（2012年）\n",
    "\n",
    "**AlexNet**のImageNet勝利が転機となりました：\n",
    "\n",
    "- GPUによる並列計算の恩恵\n",
    "- Dropoutによる過学習の防止\n",
    "- ReLUによる勾配問題の解決\n",
    "- Data Augmentationによる汎化性能向上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.2 LeNet-5：最初の実用的CNN（1998年）\n",
    "\n",
    "Yann LeCunによって開発され、手書き数字認識で成功を収めました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.2.1 LeNet-5のアーキテクチャ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "【LeNet-5 アーキテクチャ】\n",
    "\n",
    "入力: 32×32グレースケール画像\n",
    "    ↓\n",
    "C1: 畳み込み (6フィルタ, 5×5, stride=1) → tanh\n",
    "    ↓ 28×28×6\n",
    "S2: 平均プーリング (2×2, stride=2)\n",
    "    ↓ 14×14×6\n",
    "C3: 畳み込み (16フィルタ, 5×5, stride=1) → tanh\n",
    "    ↓ 10×10×16\n",
    "S4: 平均プーリング (2×2, stride=2)\n",
    "    ↓ 5×5×16\n",
    "C5: 畳み込み (120フィルタ, 5×5, stride=1) → tanh\n",
    "    ↓ 1×1×120\n",
    "F6: 全結合 (84ユニット) → tanh\n",
    "    ↓\n",
    "出力: 全結合 (10ユニット) → RBF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.2.2 LeNet-5の特徴と革新点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 層の役割の体系化\n",
    "\n",
    "| 層 | 役割 | 特徴 |\n",
    "|---|---|---|\n",
    "| 畳み込み | 特徴抽出 | 局所的特徴の検出 |\n",
    "| プーリング | 次元削減 | 位置不変性の獲得 |\n",
    "| 全結合 | 分類 | 高レベル特徴の統合 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. サブサンプリングの概念\n",
    "\n",
    "平均プーリングにより、微小な位置変化への不変性を獲得：\n",
    "\n",
    "```python\n",
    "def average_pooling(X, pool_size=2, stride=2):\n",
    "    \"\"\"平均プーリングの実装\"\"\"\n",
    "    N, C, H, W = X.shape\n",
    "    out_h = (H - pool_size) // stride + 1\n",
    "    out_w = (W - pool_size) // stride + 1\n",
    "    \n",
    "    out = np.zeros((N, C, out_h, out_w))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    h_start = i * stride\n",
    "                    h_end = h_start + pool_size\n",
    "                    w_start = j * stride\n",
    "                    w_end = w_start + pool_size\n",
    "                    \n",
    "                    # 平均を計算\n",
    "                    out[n, c, i, j] = np.mean(X[n, c, h_start:h_end, w_start:w_end])\n",
    "    \n",
    "    return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Practice (2 hours)\n",
    "\n",
    "## Exercise 25.1: LeNet-5の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# MNISTデータセットの読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 前処理\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "X_train = np.expand_dims(X_train, axis=1)  # (N, 1, 28, 28) -> (N, 1, 32, 32)にパディング\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# パディングして32x32に\n",
    "X_train = np.pad(X_train, ((0,0), (0,0), (2,2), (2,2)), 'constant')\n",
    "X_test = np.pad(X_test, ((0,0), (0,0), (2,2), (2,2)), 'constant')\n",
    "\n",
    "# ラベルをone-hotエンコーディング\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"訓練データ形状: {X_train.shape}\")\n",
    "print(f\"テストデータ形状: {X_test.shape}\")\n",
    "print(f\"ラベル形状: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Self-Check (理解度確認)\n",
    "\n",
    "本日の学習内容を確認しましょう：\n",
    "\n",
    "## 基礎知識\n",
    "- [ ] LeNet-5のアーキテクチャ（7層）を理解した\n",
    "- [ ] AlexNetの革新点（ReLU, Dropout, Data Augmentation）を理解した\n",
    "- [ ] CNN層の役割（畳み込み、プーリング、全結合）を説明できる\n",
    "\n",
    "## 深層化の理解\n",
    "- [ ] 層の深さと表現力の関係を理解した\n",
    "- [ ] 深層化の課題（勾配消失、計算コスト）を理解した\n",
    "- [ ] ResNetなどの解決策の概要を理解した\n",
    "\n",
    "## 最適化技術\n",
    "- [ ] Xavier/He初期化の違いを理解した\n",
    "- [ ] 学習率スケジュールの種類と効果を理解した\n",
    "- [ ] SGD, SGD+Momentum, Adamの特性を理解した\n",
    "\n",
    "## 実践力\n",
    "- [ ] LeNet-5の基本構造を実装した\n",
    "- [ ] AlexNetのモデルを作成した\n",
    "- [ ] 最適化アルゴリズムを比較した\n",
    "- [ ] 学習率の影響を可視化した\n",
    "\n",
    "**お疲れ様でした！** Day 25はこれで終了です。\n",
    "\n",
    "次回（Day 26）は「転移学習（Transfer Learning）」を学びます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
